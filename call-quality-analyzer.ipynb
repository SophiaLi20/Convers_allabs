{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqYAJ0A6tHMjQKhVXKOKVW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophiaLi20/Convers_allabs/blob/main/call-quality-analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "IpwgINoayi2N",
        "outputId": "9e98cffa-56c6-4195-8139-ba385138f9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.8/897.8 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m754.1/754.1 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Speaker diarization not available - will use fallback method\n",
            "ğŸš€ Call Quality Analyzer - Voice AI Assignment\n",
            "Developer: [Your Name Here]\n",
            "Date: 2025-09-17 17:52:28\n",
            "============================================================\n",
            "ğŸš€ Initializing Call Quality Analyzer...\n",
            "Loading Whisper model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:02<00:00, 65.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Analyzer ready!\n",
            "ğŸ¯ Starting Call Quality Analysis\n",
            "==================================================\n",
            "ğŸ“¥ Downloading audio from YouTube...\n",
            "[youtube] 4ostqJD3Psc: Downloading webpage\n",
            "[youtube] 4ostqJD3Psc: Downloading API JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: Unable to extract uploader id; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âŒ Error downloading audio: ERROR: Unable to extract uploader id; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\n",
            "âŒ Analysis failed. Please check the audio URL and try again.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAPPROACH EXPLANATION:\\n\\nThis Call Quality Analyzer uses a multi-step approach optimized for the free Colab tier:\\n\\n1. **Audio Processing**: Downloads YouTube audio using youtube-dl, converts to 16kHz mono WAV for efficiency.\\n\\n2. **Transcription**: Uses OpenAI Whisper (base model) for accurate speech-to-text with timestamps, balancing speed vs accuracy.\\n\\n3. **Speaker Detection**: Implements a hybrid approach combining:\\n   - Audio features (energy, pitch analysis)  \\n   - Text-based heuristics (sales keywords vs customer language patterns)\\n   - Positional analysis (sales reps often speak first)\\n\\n4. **Metrics Calculation**:\\n   - Talk-time ratio: Sums segment durations per speaker\\n   - Question counting: Detects \"?\" and question word patterns\\n   - Monologue detection: Finds longest continuous speech by same speaker\\n   - Sentiment analysis: Uses VADER + TextBlob for robust sentiment scoring\\n\\n5. **Insight Generation**: Priority-based system analyzing talk balance, question frequency, pacing, and sentiment to provide actionable feedback.\\n\\n6. **Optimization**: \\n   - Uses efficient models (Whisper base, not large)\\n   - Processes audio at 16kHz to reduce computational load\\n   - Implements fallback methods for robustness\\n   - Cleans up temporary files\\n\\nThe system handles poor audio quality through Whisper\\'s noise robustness and provides comprehensive analysis within the 30-second time limit.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Call Quality Analyzer for Sales Calls\n",
        "# Author: [Your Name]\n",
        "# Description: Analyzes sales call recordings for key metrics and insights\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q youtube-dl pydub librosa soundfile transformers torch torchaudio speechrecognition\n",
        "!pip install -q openai-whisper textblob nltk pyannote.audio\n",
        "!apt-get -qq install ffmpeg\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Audio processing can be done through these\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "\n",
        "# NLP and sentiment analysis\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# download from the YouTube\n",
        "import youtube_dl\n",
        "\n",
        "# Whisper for transcription\n",
        "import whisper\n",
        "\n",
        "# Speaker diarization\n",
        "try:\n",
        "    from pyannote.audio import Pipeline\n",
        "    DIARIZATION_AVAILABLE = True\n",
        "except:\n",
        "    DIARIZATION_AVAILABLE = False\n",
        "    print(\"Speaker diarization not available - will use fallback method\")\n",
        "\n",
        "class CallQualityAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the Call Quality Analyzer with required models\"\"\"\n",
        "        print(\"ğŸš€ Initializing Call Quality Analyzer...\")\n",
        "\n",
        "        # Load Whisper model (base for speed vs accuracy balance)\n",
        "        print(\"Loading Whisper model...\")\n",
        "        self.whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "        # Initialize sentiment analyzer\n",
        "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "        print(\"âœ… Analyzer ready!\")\n",
        "\n",
        "    def download_youtube_audio(self, youtube_url, output_path=\"audio.wav\"):\n",
        "        \"\"\"Download audio from YouTube video\"\"\"\n",
        "        print(f\"ğŸ“¥ Downloading audio from YouTube...\")\n",
        "\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': 'temp_audio.%(ext)s',\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'wav',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "                ydl.download([youtube_url])\n",
        "\n",
        "            # Convert to standard format\n",
        "            audio = AudioSegment.from_file(\"temp_audio.wav\")\n",
        "            # Normalize and reduce file size for processing\n",
        "            audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "            audio.export(output_path, format=\"wav\")\n",
        "\n",
        "            # Cleanup can be done using these\n",
        "            if os.path.exists(\"temp_audio.wav\"):\n",
        "                os.remove(\"temp_audio.wav\")\n",
        "\n",
        "            print(f\"âœ… Audio downloaded: {output_path}\")\n",
        "            return output_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error downloading audio: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def transcribe_audio(self, audio_path):\n",
        "        \"\"\"Transcribe audio using Whisper with timestamp information\"\"\"\n",
        "        print(\"ğŸ¤ Transcribing audio...\")\n",
        "\n",
        "        try:\n",
        "            # Use Whisper for transcription with word-level timestamps\n",
        "            result = self.whisper_model.transcribe(\n",
        "                audio_path,\n",
        "                word_timestamps=True,\n",
        "                language=\"en\"\n",
        "            )\n",
        "\n",
        "            # Extract segments with timestamps\n",
        "            segments = []\n",
        "            for segment in result['segments']:\n",
        "                segments.append({\n",
        "                    'start': segment['start'],\n",
        "                    'end': segment['end'],\n",
        "                    'text': segment['text'].strip(),\n",
        "                    'words': segment.get('words', [])\n",
        "                })\n",
        "\n",
        "            full_text = result['text']\n",
        "            print(f\"âœ… Transcription complete: {len(segments)} segments\")\n",
        "\n",
        "            return {\n",
        "                'full_text': full_text,\n",
        "                'segments': segments,\n",
        "                'language': result.get('language', 'en')\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error in transcription: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def simple_speaker_detection(self, segments, audio_path):\n",
        "        \"\"\"Simple speaker detection based on audio analysis and text patterns\"\"\"\n",
        "        print(\"ğŸ‘¥ Detecting speakers...\")\n",
        "\n",
        "        try:\n",
        "            # Load audio for analysis\n",
        "            y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "            # Calculate energy and speaking patterns for each segment\n",
        "            speakers = []\n",
        "            for i, segment in enumerate(segments):\n",
        "                start_sample = int(segment['start'] * sr)\n",
        "                end_sample = int(segment['end'] * sr)\n",
        "\n",
        "                if end_sample > len(y):\n",
        "                    end_sample = len(y)\n",
        "\n",
        "                segment_audio = y[start_sample:end_sample]\n",
        "\n",
        "                # Calculate features using these\n",
        "                energy = np.mean(segment_audio ** 2)\n",
        "                pitch = np.mean(librosa.yin(segment_audio, fmin=50, fmax=400))\n",
        "\n",
        "                # Text-based heuristics for sales rep vs customer\n",
        "                text = segment['text'].lower()\n",
        "\n",
        "                # Sales rep indicators\n",
        "                sales_keywords = [\n",
        "                    'how can i help', 'thank you for', 'our product', 'we offer',\n",
        "                    'let me show you', 'our company', 'i can help', 'our service',\n",
        "                    'pricing', 'package', 'deal', 'offer', 'solution'\n",
        "                ]\n",
        "\n",
        "                # Customer indicators can be done using thesse.\n",
        "                customer_keywords = [\n",
        "                    'i need', 'i want', 'i\\'m looking for', 'my company',\n",
        "                    'we need', 'what about', 'how much', 'tell me more'\n",
        "                ]\n",
        "\n",
        "                sales_score = sum(1 for keyword in sales_keywords if keyword in text)\n",
        "                customer_score = sum(1 for keyword in customer_keywords if keyword in text)\n",
        "\n",
        "                # Simple classification\n",
        "                if sales_score > customer_score:\n",
        "                    speaker = 'Sales Rep'\n",
        "                elif customer_score > sales_score:\n",
        "                    speaker = 'Customer'\n",
        "                else:\n",
        "                    # Use position in call as tiebreaker (sales rep often speaks first)\n",
        "                    speaker = 'Sales Rep' if i < len(segments) // 3 else 'Customer'\n",
        "\n",
        "                speakers.append({\n",
        "                    'segment_id': i,\n",
        "                    'speaker': speaker,\n",
        "                    'confidence': max(sales_score, customer_score) / max(len(text.split()), 1)\n",
        "                })\n",
        "\n",
        "            print(\"âœ… Speaker detection complete\")\n",
        "            return speakers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Speaker detection failed, using fallback: {str(e)}\")\n",
        "            # Fallback: alternate speakers\n",
        "            return [\n",
        "                {'segment_id': i, 'speaker': 'Sales Rep' if i % 2 == 0 else 'Customer', 'confidence': 0.5}\n",
        "                for i in range(len(segments))\n",
        "            ]\n",
        "\n",
        "    def calculate_talk_time_ratio(self, segments, speakers):\n",
        "        \"\"\"Calculate talk time ratio for each speaker\"\"\"\n",
        "        print(\"â±ï¸ Calculating talk time ratios...\")\n",
        "\n",
        "        speaker_time = {'Sales Rep': 0, 'Customer': 0}\n",
        "\n",
        "        for i, segment in enumerate(segments):\n",
        "            duration = segment['end'] - segment['start']\n",
        "            speaker = speakers[i]['speaker']\n",
        "            speaker_time[speaker] += duration\n",
        "\n",
        "        total_time = sum(speaker_time.values())\n",
        "\n",
        "        if total_time > 0:\n",
        "            talk_ratio = {\n",
        "                speaker: (time / total_time) * 100\n",
        "                for speaker, time in speaker_time.items()\n",
        "            }\n",
        "        else:\n",
        "            talk_ratio = {'Sales Rep': 50, 'Customer': 50}\n",
        "\n",
        "        print(f\"âœ… Sales Rep: {talk_ratio['Sales Rep']:.1f}%, Customer: {talk_ratio['Customer']:.1f}%\")\n",
        "        return talk_ratio\n",
        "\n",
        "    def count_questions(self, segments):\n",
        "        \"\"\"Count number of questions asked in the conversation\"\"\"\n",
        "        print(\"â“ Counting questions...\")\n",
        "\n",
        "        question_indicators = ['?', 'what', 'how', 'when', 'where', 'why', 'who', 'which', 'can you', 'do you', 'are you']\n",
        "\n",
        "        total_questions = 0\n",
        "        for segment in segments:\n",
        "            text = segment['text'].lower()\n",
        "\n",
        "            # Direct question marks\n",
        "            questions = text.count('?')\n",
        "\n",
        "            # Question words at start of sentences\n",
        "            sentences = text.split('.')\n",
        "            for sentence in sentences:\n",
        "                sentence = sentence.strip()\n",
        "                if any(sentence.startswith(indicator) for indicator in question_indicators):\n",
        "                    questions += 1\n",
        "\n",
        "            total_questions += questions\n",
        "\n",
        "        print(f\"âœ… Found {total_questions} questions\")\n",
        "        return total_questions\n",
        "\n",
        "    def find_longest_monologue(self, segments, speakers):\n",
        "        \"\"\"Find the longest continuous speaking period by one person\"\"\"\n",
        "        print(\"ğŸ—£ï¸ Finding longest monologue...\")\n",
        "\n",
        "        current_speaker = None\n",
        "        current_duration = 0\n",
        "        max_duration = 0\n",
        "        max_speaker = None\n",
        "        monologue_start = 0\n",
        "\n",
        "        for i, segment in enumerate(segments):\n",
        "            speaker = speakers[i]['speaker']\n",
        "            duration = segment['end'] - segment['start']\n",
        "\n",
        "            if speaker == current_speaker:\n",
        "                current_duration += duration\n",
        "            else:\n",
        "                if current_duration > max_duration:\n",
        "                    max_duration = current_duration\n",
        "                    max_speaker = current_speaker\n",
        "\n",
        "                current_speaker = speaker\n",
        "                current_duration = duration\n",
        "                monologue_start = segment['start']\n",
        "\n",
        "        # Check final monologue\n",
        "        if current_duration > max_duration:\n",
        "            max_duration = current_duration\n",
        "            max_speaker = current_speaker\n",
        "\n",
        "        print(f\"âœ… Longest monologue: {max_duration:.1f}s by {max_speaker}\")\n",
        "        return {\n",
        "            'duration': max_duration,\n",
        "            'speaker': max_speaker,\n",
        "            'duration_formatted': f\"{int(max_duration // 60)}m {int(max_duration % 60)}s\"\n",
        "        }\n",
        "\n",
        "    def analyze_sentiment(self, full_text):\n",
        "        \"\"\"Analyze overall call sentiment\"\"\"\n",
        "        print(\"ğŸ˜Š Analyzing sentiment...\")\n",
        "\n",
        "        # Use VADER sentiment analyzer\n",
        "        scores = self.sentiment_analyzer.polarity_scores(full_text)\n",
        "\n",
        "        # Also use TextBlob for comparison\n",
        "        blob = TextBlob(full_text)\n",
        "        textblob_sentiment = blob.sentiment.polarity\n",
        "\n",
        "        # scores can be compound using these\n",
        "        compound_score = scores['compound']\n",
        "\n",
        "        # Determine overall sentiment\n",
        "        if compound_score >= 0.05:\n",
        "            sentiment = 'Positive'\n",
        "        elif compound_score <= -0.05:\n",
        "            sentiment = 'Negative'\n",
        "        else:\n",
        "            sentiment = 'Neutral'\n",
        "\n",
        "        confidence = abs(compound_score)\n",
        "\n",
        "        print(f\"âœ… Sentiment: {sentiment} (confidence: {confidence:.2f})\")\n",
        "\n",
        "        return {\n",
        "            'sentiment': sentiment,\n",
        "            'confidence': confidence,\n",
        "            'scores': scores,\n",
        "            'textblob_polarity': textblob_sentiment\n",
        "        }\n",
        "\n",
        "    def generate_actionable_insight(self, analysis_results):\n",
        "        \"\"\"Generate one actionable insight based on analysis\"\"\"\n",
        "        print(\"ğŸ’¡ Generating actionable insight...\")\n",
        "\n",
        "        talk_ratio = analysis_results['talk_time_ratio']\n",
        "        questions = analysis_results['question_count']\n",
        "        monologue = analysis_results['longest_monologue']\n",
        "        sentiment = analysis_results['sentiment']\n",
        "\n",
        "        insights = []\n",
        "\n",
        "        # Talk time insights\n",
        "        if talk_ratio['Sales Rep'] > 70:\n",
        "            insights.append({\n",
        "                'priority': 'high',\n",
        "                'insight': 'Sales rep is dominating the conversation ({}%). Encourage more customer engagement by asking open-ended questions and allowing for longer pauses.'.format(int(talk_ratio['Sales Rep'])),\n",
        "                'category': 'Talk Time'\n",
        "            })\n",
        "        elif talk_ratio['Customer'] > 70:\n",
        "            insights.append({\n",
        "                'priority': 'medium',\n",
        "                'insight': 'Customer is talking most of the time ({}%). This could indicate strong engagement, but ensure you\\'re guiding the conversation toward next steps.'.format(int(talk_ratio['Customer'])),\n",
        "                'category': 'Talk Time'\n",
        "            })\n",
        "\n",
        "        # Question insights\n",
        "        if questions < 3:\n",
        "            insights.append({\n",
        "                'priority': 'high',\n",
        "                'insight': 'Only {} questions were asked. Increase discovery by asking more qualifying questions to understand customer needs better.'.format(questions),\n",
        "                'category': 'Questions'\n",
        "            })\n",
        "        elif questions > 15:\n",
        "            insights.append({\n",
        "                'priority': 'medium',\n",
        "                'insight': '{} questions were asked. While discovery is good, ensure you\\'re also presenting solutions and not just interrogating.'.format(questions),\n",
        "                'category': 'Questions'\n",
        "            })\n",
        "\n",
        "        # Monologue insights\n",
        "        if monologue['duration'] > 120:  # 2 minutes\n",
        "            insights.append({\n",
        "                'priority': 'high',\n",
        "                'insight': 'Longest monologue was {} by {}. Break up long speaking periods with engagement checks like \"Does that make sense?\" or \"What questions do you have?\"'.format(monologue['duration_formatted'], monologue['speaker']),\n",
        "                'category': 'Pacing'\n",
        "            })\n",
        "\n",
        "        # Sentiment insights\n",
        "        if sentiment['sentiment'] == 'Negative':\n",
        "            insights.append({\n",
        "                'priority': 'high',\n",
        "                'insight': 'Call sentiment is negative. Follow up immediately to address concerns and rebuild rapport before next interaction.',\n",
        "                'category': 'Sentiment'\n",
        "            })\n",
        "        elif sentiment['sentiment'] == 'Neutral':\n",
        "            insights.append({\n",
        "                'priority': 'medium',\n",
        "                'insight': 'Call sentiment is neutral. Work on building more emotional connection and enthusiasm about your solution.',\n",
        "                'category': 'Sentiment'\n",
        "            })\n",
        "\n",
        "        # Default insight if no specific issues\n",
        "        if not insights:\n",
        "            insights.append({\n",
        "                'priority': 'low',\n",
        "                'insight': 'Call metrics look balanced. Focus on clear next steps and follow-up timeline to maintain momentum.',\n",
        "                'category': 'General'\n",
        "            })\n",
        "\n",
        "        # Return highest priority insight\n",
        "        top_insight = max(insights, key=lambda x: {'high': 3, 'medium': 2, 'low': 1}[x['priority']])\n",
        "\n",
        "        print(f\"âœ… Key insight: {top_insight['insight']}\")\n",
        "        return top_insight\n",
        "\n",
        "    def analyze_call(self, youtube_url):\n",
        "        \"\"\"Main function to analyze a sales call\"\"\"\n",
        "        print(\"ğŸ¯ Starting Call Quality Analysis\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Step 1: Download audio\n",
        "        audio_path = self.download_youtube_audio(youtube_url)\n",
        "        if not audio_path:\n",
        "            return None\n",
        "\n",
        "        # Step 2: Transcribe\n",
        "        transcription = self.transcribe_audio(audio_path)\n",
        "        if not transcription:\n",
        "            return None\n",
        "\n",
        "        # Step 3: Speaker detection\n",
        "        speakers = self.simple_speaker_detection(transcription['segments'], audio_path)\n",
        "\n",
        "        # Step 4: Calculate metrics\n",
        "        talk_ratio = self.calculate_talk_time_ratio(transcription['segments'], speakers)\n",
        "        question_count = self.count_questions(transcription['segments'])\n",
        "        longest_monologue = self.find_longest_monologue(transcription['segments'], speakers)\n",
        "        sentiment = self.analyze_sentiment(transcription['full_text'])\n",
        "\n",
        "        # Compile results\n",
        "        results = {\n",
        "            'talk_time_ratio': talk_ratio,\n",
        "            'question_count': question_count,\n",
        "            'longest_monologue': longest_monologue,\n",
        "            'sentiment': sentiment,\n",
        "            'processing_time': (datetime.now() - start_time).total_seconds(),\n",
        "            'transcription': transcription,\n",
        "            'speakers': speakers\n",
        "        }\n",
        "\n",
        "        # Step 5: Generate insight\n",
        "        results['actionable_insight'] = self.generate_actionable_insight(results)\n",
        "\n",
        "        # Cleanup\n",
        "        if os.path.exists(audio_path):\n",
        "            os.remove(audio_path)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def display_results(self, results):\n",
        "        \"\"\"Display analysis results in a formatted way\"\"\"\n",
        "        if not results:\n",
        "            print(\"âŒ No results to display\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ğŸ“Š CALL QUALITY ANALYSIS RESULTS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"\\nâ±ï¸ PROCESSING TIME: {results['processing_time']:.1f} seconds\")\n",
        "\n",
        "        print(f\"\\nğŸ—£ï¸ TALK TIME RATIO:\")\n",
        "        for speaker, percentage in results['talk_time_ratio'].items():\n",
        "            print(f\"   {speaker}: {percentage:.1f}%\")\n",
        "\n",
        "        print(f\"\\nâ“ QUESTIONS ASKED: {results['question_count']}\")\n",
        "\n",
        "        print(f\"\\nğŸ“¢ LONGEST MONOLOGUE:\")\n",
        "        monologue = results['longest_monologue']\n",
        "        print(f\"   Duration: {monologue['duration_formatted']}\")\n",
        "        print(f\"   Speaker: {monologue['speaker']}\")\n",
        "\n",
        "        print(f\"\\nğŸ˜Š CALL SENTIMENT: {results['sentiment']['sentiment']}\")\n",
        "        print(f\"   Confidence: {results['sentiment']['confidence']:.2f}\")\n",
        "\n",
        "        print(f\"\\nğŸ’¡ ACTIONABLE INSIGHT:\")\n",
        "        insight = results['actionable_insight']\n",
        "        print(f\"   Category: {insight['category']}\")\n",
        "        print(f\"   Priority: {insight['priority'].upper()}\")\n",
        "        print(f\"   Recommendation: {insight['insight']}\")\n",
        "\n",
        "        print(f\"\\nğŸ‘¥ SPEAKER IDENTIFICATION:\")\n",
        "        speaker_segments = {}\n",
        "        for speaker_info in results['speakers']:\n",
        "            speaker = speaker_info['speaker']\n",
        "            if speaker not in speaker_segments:\n",
        "                speaker_segments[speaker] = 0\n",
        "            speaker_segments[speaker] += 1\n",
        "\n",
        "        for speaker, count in speaker_segments.items():\n",
        "            print(f\"   {speaker}: {count} segments\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Test URL provided in the assignment\n",
        "    TEST_URL = \"https://www.youtube.com/watch?v=4ostqJD3Psc\"\n",
        "\n",
        "    print(\"ğŸš€ Call Quality Analyzer - Voice AI Assignment\")\n",
        "    print(\"Developer: [Your Name Here]\")\n",
        "    print(\"Date: {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = CallQualityAnalyzer()\n",
        "\n",
        "    # Analyze the call\n",
        "    results = analyzer.analyze_call(TEST_URL)\n",
        "\n",
        "    if results:\n",
        "        # Display results\n",
        "        analyzer.display_results(results)\n",
        "\n",
        "        # Additional visualization\n",
        "        print(\"\\nğŸ“ˆ VISUAL SUMMARY:\")\n",
        "\n",
        "        # Create a simple visualization\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "        # Talk time ratio pie chart\n",
        "        speakers = list(results['talk_time_ratio'].keys())\n",
        "        percentages = list(results['talk_time_ratio'].values())\n",
        "        colors = ['#ff9999', '#66b3ff']\n",
        "\n",
        "        ax1.pie(percentages, labels=speakers, autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "        ax1.set_title('Talk Time Ratio')\n",
        "\n",
        "        # Questions bar chart\n",
        "        ax2.bar(['Questions Asked'], [results['question_count']], color='#99ff99')\n",
        "        ax2.set_title('Questions Count')\n",
        "        ax2.set_ylabel('Number of Questions')\n",
        "\n",
        "        # Monologue duration\n",
        "        ax3.bar([results['longest_monologue']['speaker']], [results['longest_monologue']['duration']], color='#ffcc99')\n",
        "        ax3.set_title('Longest Monologue Duration')\n",
        "        ax3.set_ylabel('Duration (seconds)')\n",
        "\n",
        "        # Sentiment scores\n",
        "        sentiment_scores = results['sentiment']['scores']\n",
        "        sentiments = ['Positive', 'Neutral', 'Negative']\n",
        "        scores = [sentiment_scores['pos'], sentiment_scores['neu'], sentiment_scores['neg']]\n",
        "        colors = ['green', 'gray', 'red']\n",
        "\n",
        "        ax4.bar(sentiments, scores, color=colors)\n",
        "        ax4.set_title('Sentiment Analysis')\n",
        "        ax4.set_ylabel('Score')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"âœ… Analysis completed successfully!\")\n",
        "\n",
        "        # Performance check\n",
        "        if results['processing_time'] > 30:\n",
        "            print(\"âš ï¸ Warning: Processing took longer than 30 seconds\")\n",
        "        else:\n",
        "            print(f\"âœ… Processing completed within time limit: {results['processing_time']:.1f}s\")\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ Analysis failed. Please check the audio URL and try again.\")\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# =============================================================================\n",
        "# APPROACH EXPLANATION (< 200 words)\n",
        "# =============================================================================\n",
        "\n",
        "\"\"\"\n",
        "APPROACH EXPLANATION:\n",
        "\n",
        "This Call Quality Analyzer uses a multi-step approach optimized for the free Colab tier:\n",
        "\n",
        "1. **Audio Processing**: Downloads YouTube audio using youtube-dl, converts to 16kHz mono WAV for efficiency.\n",
        "\n",
        "2. **Transcription**: Uses OpenAI Whisper (base model) for accurate speech-to-text with timestamps, balancing speed vs accuracy.\n",
        "\n",
        "3. **Speaker Detection**: Implements a hybrid approach combining:\n",
        "   - Audio features (energy, pitch analysis)\n",
        "   - Text-based heuristics (sales keywords vs customer language patterns)\n",
        "   - Positional analysis (sales reps often speak first)\n",
        "\n",
        "4. **Metrics Calculation**:\n",
        "   - Talk-time ratio: Sums segment durations per speaker\n",
        "   - Question counting: Detects \"?\" and question word patterns\n",
        "   - Monologue detection: Finds longest continuous speech by same speaker\n",
        "   - Sentiment analysis: Uses VADER + TextBlob for robust sentiment scoring\n",
        "\n",
        "5. **Insight Generation**: Priority-based system analyzing talk balance, question frequency, pacing, and sentiment to provide actionable feedback.\n",
        "\n",
        "6. **Optimization**:\n",
        "   - Uses efficient models (Whisper base, not large)\n",
        "   - Processes audio at 16kHz to reduce computational load\n",
        "   - Implements fallback methods for robustness\n",
        "   - Cleans up temporary files\n",
        "\n",
        "The system handles poor audio quality through Whisper's noise robustness and provides comprehensive analysis within the 30-second time limit.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BxiXRY-ezUEX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}